Note: no use lstm in predictor 
roberta-base
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 512, 'p_mlp_dim': 768, 'epoches': 7, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 5, 's_lr': 0.0001, 'b_lr': 7e-05, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.4, 'word_drop_rate': 0.1, 'task_reward': 'logit', 'perfomance_reward_weight': 0.1, 'ctx_sim_reward_weight': 0.08, 'knowledge_reward_weight': 0.1}
 Test F1: 0.6207357859531772
Sub: True - Mul: TrueT 
F1: 0.5875862068965517 
CM: 
 [[101   4   0   1   0  53]
 [  4 109   1   4   0  63]
 [  1   0   1   0   0  14]
 [  1   0   0   2   0  22]
 [  4   1   1   0   0  22]
 [ 36  30   6   9   0 173]] 
Time: 2021-07-12 06:41:53.303845 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-base
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 768, 'p_mlp_dim': 768, 'epoches': 5, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 1, 's_lr': 5e-05, 'b_lr': 5e-05, 'm_lr': 3e-05, 'b_lr_decay_rate': 0.5, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.7, 'ctx_sim_reward_weight': 0.05, 'knowledge_reward_weight': 0.1}
 Test F1: 0.6348039215686274
Sub: True - Mul: TrueT 
F1: 0.6107470511140235 
CM: 
 [[115   4   0   0   1  39]
 [  5 111   3   3   2  57]
 [  1   0   4   0   1  10]
 [  2   1   0   1   1  20]
 [  4   2   1   0   2  19]
 [ 40  26   7  15   2 164]] 
Time: 2021-07-12 07:34:19.919772 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-base
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 768, 'p_mlp_dim': 1024, 'epoches': 5, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 3, 's_lr': 5e-05, 'b_lr': 5e-05, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.5, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.5, 'ctx_sim_reward_weight': 0.01, 'knowledge_reward_weight': 0.1}
 Test F1: 0.6413921690490988
Sub: True - Mul: TrueT 
F1: 0.5981554677206852 
CM: 
 [[108   4   0   1   0  46]
 [  6 109   5   6   0  55]
 [  1   0   3   0   1  11]
 [  1   0   0   5   1  18]
 [  4   2   1   0   2  19]
 [ 38  29   8  15   0 164]] 
Time: 2021-07-12 09:15:35.579148 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-base
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 768, 'p_mlp_dim': 768, 'epoches': 5, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 5, 's_lr': 0.0001, 'b_lr': 3e-05, 'm_lr': 3e-05, 'b_lr_decay_rate': 0.6, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.5, 'ctx_sim_reward_weight': 0.08, 'knowledge_reward_weight': 0.7}
 Test F1: 0.6001405481377373
Sub: True - Mul: TrueT 
F1: 0.5882352941176472 
CM: 
 [[101   4   0   0   0  54]
 [  5 104   0   0   0  72]
 [  1   1   0   0   0  14]
 [  2   1   0   0   0  22]
 [  3   1   0   0   0  24]
 [ 34  30   1   0   0 189]] 
Time: 2021-07-12 09:43:26.697858 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-base
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 768, 'p_mlp_dim': 768, 'epoches': 5, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 3, 's_lr': 0.0001, 'b_lr': 3e-05, 'm_lr': 3e-05, 'b_lr_decay_rate': 0.6, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.7, 'ctx_sim_reward_weight': 0.03, 'knowledge_reward_weight': 0.5}
 Test F1: 0.6042670337233311
Sub: True - Mul: TrueT 
F1: 0.5797101449275363 
CM: 
 [[100   3   0   0   0  56]
 [  6  99   0   0   0  76]
 [  1   0   0   0   0  15]
 [  2   0   0   1   0  22]
 [  6   1   0   0   0  21]
 [ 43  19   0   0   0 192]] 
Time: 2021-07-12 10:39:37.652140 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-base
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 768, 'p_mlp_dim': 512, 'epoches': 7, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 1, 's_lr': 9.639868035232342e-06, 'b_lr': 5.308712477492115e-05, 'm_lr': 2.201162048010001e-05, 'b_lr_decay_rate': 0.7, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 1, 'ctx_sim_reward_weight': 0.08, 'knowledge_reward_weight': 0.1}
 Test F1: 0.6014445173998686
Sub: True - Mul: TrueT 
F1: 0.6053333333333333 
CM: 
 [[105   2   0   0   0  52]
 [  7 120   0   0   0  54]
 [  1   1   1   0   0  13]
 [  1   1   0   1   0  22]
 [  3   4   0   0   0  21]
 [ 46  48   0   0   0 160]] 
Time: 2021-07-12 22:54:29.630763 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-base
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 768, 'p_mlp_dim': 512, 'epoches': 7, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 1, 's_lr': 1.4074213389260786e-05, 'b_lr': 6.366900541701643e-05, 'm_lr': 2.348047910725692e-05, 'b_lr_decay_rate': 0.7, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 1, 'ctx_sim_reward_weight': 0.05, 'knowledge_reward_weight': 0.1}
 Test F1: 0.622701331642359
Sub: True - Mul: TrueT 
F1: 0.6059817945383614 
CM: 
 [[119   2   0   0   0  38]
 [  7 105   3   5   0  61]
 [  1   1   4   0   0  10]
 [  3   0   0   4   0  18]
 [  5   4   0   0   1  18]
 [ 39  28   9  18   2 158]] 
Time: 2021-07-12 23:26:42.759090 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-base
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 768, 'p_mlp_dim': 768, 'epoches': 7, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 1, 's_lr': 2.9717858577215478e-05, 'b_lr': 1.7103048613585138e-05, 'm_lr': 2.4234857176309643e-05, 'b_lr_decay_rate': 0.6, 'word_drop_rate': 0.1, 'task_reward': 'logit', 'perfomance_reward_weight': 1, 'ctx_sim_reward_weight': 0.05, 'knowledge_reward_weight': 0.1}
 Test F1: 0.6010568031704094
Sub: True - Mul: TrueT 
F1: 0.579172610556348 
CM: 
 [[ 97   3   0   0   0  59]
 [  5 105   0   0   0  71]
 [  1   1   1   0   0  13]
 [  3   0   0   0   0  22]
 [  4   2   0   0   0  22]
 [ 41  27   1   1   0 184]] 
Time: 2021-07-12 23:58:58.907913 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-base
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 768, 'p_mlp_dim': 768, 'epoches': 7, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 1, 's_lr': 1.0887904314200243e-05, 'b_lr': 2.7307332822172235e-05, 'm_lr': 2.135308189879313e-05, 'b_lr_decay_rate': 0.6, 'word_drop_rate': 0.1, 'task_reward': 'logit', 'perfomance_reward_weight': 1, 'ctx_sim_reward_weight': 0.05, 'knowledge_reward_weight': 0.1}
 Test F1: 0.6189873417721519
Sub: True - Mul: TrueT 
F1: 0.6077643908969209 
CM: 
 [[110   5   0   0   0  44]
 [  4 111   0   4   0  62]
 [  1   0   2   0   1  12]
 [  1   1   0   2   1  20]
 [  4   3   0   0   2  19]
 [ 39  32   6   8   1 168]] 
Time: 2021-07-13 02:07:55.676200 

 -------------------------------------------- 
roberta-base
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 512, 'p_mlp_dim': 1024, 'epoches': 7, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 3, 's_lr': 5.043705203459491e-06, 'b_lr': 2.0903775787468475e-05, 'm_lr': 7.536426374320064e-05, 'b_lr_decay_rate': 0.8, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.1, 'ctx_sim_reward_weight': 0.08, 'knowledge_reward_weight': 0.5, 'is_lstm': False, 'threshold': 1.1266065387038702}
 Test F1: 0.6134913400182316
Sub: True - Mul: TrueT 
F1: 0.6422764227642277 
CM: 
 [[142   6   0   1   3   7]
 [  7 140   6   6   3  19]
 [  3   0   7   1   3   2]
 [  1   1   0  14   1   8]
 [  8   2   1   1  13   3]
 [ 87  50  24  37   7  49]] 
Time: 2021-07-13 09:27:45.349761 

 -------------------------------------------- 
roberta-base
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 768, 'p_mlp_dim': 1024, 'epoches': 5, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 5, 's_lr': 7.833821948321228e-05, 'b_lr': 2.0217142857158814e-06, 'm_lr': 1.7619236375719945e-06, 'b_lr_decay_rate': 0.5, 'word_drop_rate': 0.1, 'task_reward': 'logit', 'perfomance_reward_weight': 0.5, 'ctx_sim_reward_weight': 0.05, 'knowledge_reward_weight': 0.7, 'is_lstm': False, 'threshold': 1.2875503299472804}
 Test F1: 0.02486402486402486
Sub: True - Mul: TrueT 
F1: 0.048327137546468404 
CM: 
 [[  0   0   0  11  21 127]
 [  0   0   0  13  21 147]
 [  0   0   0   3   2  11]
 [  0   0   0   2   2  21]
 [  0   0   0   1  11  16]
 [  0   0   0   7  35 212]] 
Time: 2021-07-13 09:53:20.380733 
