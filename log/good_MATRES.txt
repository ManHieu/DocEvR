Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 512, 'p_mlp_dim': 512, 'epoches': 3, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 5, 's_lr': 0.0001, 'b_lr': 5e-06, 'm_lr': 1e-05, 'b_lr_decay_rate': 0.4, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.5, 'ctx_sim_reward_weight': 0.01, 'knowledge_reward_weight': 0.5}
 Test F1: 0.816137566137566
Sub: True - Mul: True
 Best F1 MATRES: 0.8066346573548667 
M 
F1: 0.8066346573548667 
CM: 
 [[583  38   0   4]
 [ 69 341   0   6]
 [ 19  13   0   3]
 [101  51   0   3]] 
Time: 2021-07-08 22:55:12.522913 

 -------------------------------------------- 

 Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 768, 'p_mlp_dim': 768, 'epoches': 3, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 1, 's_lr': 1e-05, 'b_lr': 5e-06, 'm_lr': 0.0001, 'b_lr_decay_rate': 0.4, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.1, 'ctx_sim_reward_weight': 0.05, 'knowledge_reward_weight': 0.5}
 Test F1: 0.814765985497693
Sub: True - Mul: True
 Best F1 MATRES: 0.8334293948126802 
M 
F1: 0.8334293948126802 
CM: 
 [[460  24   0   4]
 [ 34 263   0   4]
 [ 18  15   0   2]
 [ 65  32   0  16]] 
Time: 2021-07-08 16:30:27.157898 

 -------------------------------------------- 

 Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 512, 'p_mlp_dim': 768, 'epoches': 7, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 5, 's_lr': 5e-05, 'b_lr': 5e-06, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.4, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.5, 'ctx_sim_reward_weight': 0.08, 'knowledge_reward_weight': 0.1}
 Test F1: 0.8129117259552043
Sub: True - Mul: True
 Best F1 MATRES: 0.8188914910226386 
M 
F1: 0.8188914910226386 
CM: 
 [[631  44   0   2]
 [ 60 417   0   3]
 [ 31  14   1   0]
 [ 81  80   0   1]] 
Time: 2021-07-08 13:59:47.084461 

 -------------------------------------------- 
 
Note: no use lstm in predictor, seed: 1741
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 512, 'p_mlp_dim': 1024, 'epoches': 3, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 5, 's_lr': 5e-05, 'b_lr': 7e-06, 'm_lr': 3e-05, 'b_lr_decay_rate': 0.3, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.5, 'ctx_sim_reward_weight': 0.03, 'knowledge_reward_weight': 0.7}
 Test F1: 0.8222811671087534
Sub: True - Mul: True
 Best F1 MATRES: 0.8057798555036123 
M 
F1: 0.8057798555036123 
CM: 
 [[611  42   0  13]
 [ 53 337   0   9]
 [ 31  20   0   1]
 [ 79  63   0  20]] 
Time: 2021-07-11 02:42:20.224730 

 -------------------------------------------- 