
 -------------------------------------------- 
MLP_lr for no bert layersHypeparameter: {'s_hidden_dim': 256, 's_mlp_dim': 256, 'p_mlp_dim': 1024, 'n_head': 16, 'epoches': 5, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 3, 's_lr': 0.0001, 'b_lr': 1e-06, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.5, 'word_drop_rate': 0.05, 'task_reward': 'logit'}
 Sub: True - Mul: True
 Best F1 MATRES: 0.7732793522267206 
M 
F1: 0.7732793522267206 
CM: 
 [[1056   82    0    4]
 [  92  654    0    5]
 [  49   30    0    2]
 [ 156   84    0    9]] 
Time: 2021-06-28 17:03:57.142101 

 -------------------------------------------- 
MLP_lr for no bert layersHypeparameter: {'s_hidden_dim': 256, 's_mlp_dim': 512, 'p_mlp_dim': 1024, 'n_head': 16, 'epoches': 5, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 3, 's_lr': 5e-05, 'b_lr': 1e-06, 'm_lr': 0.0001, 'b_lr_decay_rate': 0.5, 'word_drop_rate': 0.05, 'task_reward': 'logit'}
 Sub: True - Mul: True
 Best F1 MATRES: 0.7719298245614035 
M 
F1: 0.7719298245614035 
CM: 
 [[1035   92    0   15]
 [  78  656    0   17]
 [  38   37    0    6]
 [ 142   82    0   25]] 
Time: 2021-06-28 18:35:44.699521 
 -------------------------------------------- 
MLP_lr for no bert layersHypeparameter: {'s_hidden_dim': 256, 's_mlp_dim': 256, 'p_mlp_dim': 512, 'n_head': 16, 'epoches': 5, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 3, 's_lr': 0.0001, 'b_lr': 5e-06, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.5, 'word_drop_rate': 0.05, 'task_reward': 'logit'}
 Sub: True - Mul: True
 Best F1 MATRES: 0.7737291947818263 
M 
F1: 0.7737291947818263 
CM: 
 [[1035  107    0    0]
 [  65  685    0    1]
 [  39   42    0    0]
 [ 137  112    0    0]] 
Time: 2021-06-29 04:20:35.456908 
 -------------------------------------------- 
MLP_lr for not bert layers
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 256, 'p_mlp_dim': 768, 'n_head': 16, 'epoches': 5, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 3, 's_lr': 1e-05, 'b_lr': 1e-05, 'm_lr': 0.0001, 'b_lr_decay_rate': 0.6, 'word_drop_rate': 0.05, 'task_reward': 'logit'}
 Test F1: [0.7493887530562348]
Sub: True - Mul: True
 Best F1 MATRES: 0.7809266756635178 
M 
F1: 0.7809266756635178 
CM: 
 [[1064   72    0    6]
 [  85  663    0    3]
 [  46   31    0    4]
 [ 144   96    0    9]] 
Time: 2021-06-29 08:21:05.940224 

 -------------------------------------------- 
MLP_lr for not bert layers
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 768, 'p_mlp_dim': 1024, 'n_head': 16, 'epoches': 5, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 5, 's_lr': 5e-05, 'b_lr': 1e-05, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.6, 'word_drop_rate': 0.05, 'task_reward': 'logit'}
 Test F1: [0.7420537897310513]
Sub: True - Mul: True
 Best F1 MATRES: 0.7786774628879892 
M 
F1: 0.7786774628879892 
CM: 
 [[1078   64    0    0]
 [  99  651    0    1]
 [  50   31    0    0]
 [ 159   88    0    2]] 
Time: 2021-06-29 10:54:53.954564 
 -------------------------------------------- 
MLP_lr for not bert layers
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 768, 'p_mlp_dim': 1024, 'n_head': 16, 'epoches': 5, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 1, 's_lr': 1e-05, 'b_lr': 5e-06, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.7, 'word_drop_rate': 0.05, 'task_reward': 'logit'}
 Test F1: [0.7518337408312957]
Sub: True - Mul: True
 Best F1 MATRES: 0.7701404286770139 
M 
F1: 0.7701404286770139 
CM: 
 [[1280  114    0    3]
 [  97  795    0    2]
 [  43   47    0    1]
 [ 189  126    0    9]] 
Time: 2021-06-29 13:04:49.648032 
 -------------------------------------------- 
roberta-base
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 768, 'p_mlp_dim': 768, 'n_head': 16, 'epoches': 5, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 5, 's_lr': 1e-05, 'b_lr': 5e-06, 'm_lr': 1e-05, 'b_lr_decay_rate': 0.8, 'word_drop_rate': 0.05, 'task_reward': 'logit'}
 Test F1: [0.7163814180929096]
Sub: True - Mul: True
 Best F1 MATRES: 0.7719298245614035 
M 
F1: 0.7719298245614035 
CM: 
 [[1081   61    0    0]
 [ 116  635    0    0]
 [  53   28    0    0]
 [ 173   76    0    0]] 
Time: 2021-07-01 10:43:45.102782 

 -------------------------------------------- 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 256, 'p_mlp_dim': 768, 'n_head': 16, 'epoches': 7, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 1, 's_lr': 1e-05, 'b_lr': 5e-06, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.6, 'word_drop_rate': 0.05, 'task_reward': 'logit'}
 Test F1: [0.7493887530562348]
Sub: True - Mul: True
 Best F1 MATRES: 0.7804768331084121 
M 
F1: 0.7804768331084121 
CM: 
 [[1039  101    0    2]
 [  54  692    0    5]
 [  38   40    0    3]
 [ 129  116    0    4]] 
Time: 2021-07-01 18:08:29.785494 

 -------------------------------------------- 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 512, 'p_mlp_dim': 1024, 'n_head': 16, 'epoches': 3, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 1, 's_lr': 5e-05, 'b_lr': 1e-05, 'm_lr': 0.0001, 'b_lr_decay_rate': 0.6, 'word_drop_rate': 0.05, 'task_reward': 'logit'}
 Test F1: [0.7518337408312957]
Sub: True - Mul: True
 Best F1 MATRES: 0.7849752586594692 
M 
F1: 0.7849752586594692 
CM: 
 [[1077   65    0    0]
 [  83  668    0    0]
 [  48   30    0    3]
 [ 168   81    0    0]] 
Time: 2021-07-02 12:28:36.561908 

 -------------------------------------------- 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 256, 'p_mlp_dim': 1024, 'n_head': 16, 'epoches': 5, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 5, 's_lr': 1e-05, 'b_lr': 5e-06, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.5, 'word_drop_rate': 0.05, 'task_reward': 'logit'}
 Test F1: [0.7444987775061124]
Sub: True - Mul: True
 Best F1 MATRES: 0.7712418300653595 
M 
F1: 0.7712418300653595 
CM: 
 [[1110   84    0    0]
 [  88  660    0    2]
 [  36   37    0    0]
 [ 163  115    0    0]] 
Time: 2021-07-02 22:53:11.421114 

 -------------------------------------------- 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 512, 'p_mlp_dim': 1024, 'n_head': 16, 'epoches': 3, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 5, 's_lr': 5e-05, 'b_lr': 5e-06, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.6, 'word_drop_rate': 0.05, 'task_reward': 'logit'}
 Test F1: [0.743276283618582]
Sub: True - Mul: True
 Best F1 MATRES: 0.7844258688957325 
M 
F1: 0.7844258688957325 
CM: 
 [[1080  121    0    1]
 [  49  702    0    0]
 [  30   34    0    0]
 [ 114  141    0    1]] 
Time: 2021-07-03 02:02:54.700106 

 -------------------------------------------- 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 256, 'p_mlp_dim': 768, 'n_head': 16, 'epoches': 5, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 1, 's_lr': 5e-05, 'b_lr': 1e-05, 'm_lr': 0.0001, 'b_lr_decay_rate': 0.8, 'word_drop_rate': 0.05, 'task_reward': 'logit'}
 Test F1: [0.745721271393643]
Sub: True - Mul: True
 Best F1 MATRES: 0.7849752586594692 
M 
F1: 0.7849752586594692 
CM: 
 [[1077   65    0    0]
 [  79  668    0    4]
 [  45   36    0    0]
 [ 160   89    0    0]] 
Time: 2021-07-03 07:40:34.163489 

 -------------------------------------------- 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 768, 'p_mlp_dim': 1024, 'n_head': 16, 'epoches': 5, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 5, 's_lr': 5e-05, 'b_lr': 1e-05, 'm_lr': 1e-05, 'b_lr_decay_rate': 0.5, 'word_drop_rate': 0.05, 'task_reward': 'logit'}
 Test F1: [0.7555012224938874]
Sub: True - Mul: True
 Best F1 MATRES: 0.7732793522267206 
M 
F1: 0.7732793522267206 
CM: 
 [[1017   69    0   56]
 [  59  658    0   34]
 [  33   35    0   13]
 [ 119   86    0   44]] 
Time: 2021-07-03 16:30:10.661539 

 -------------------------------------------- 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 256, 'p_mlp_dim': 768, 'n_head': 16, 'epoches': 5, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 1, 's_lr': 5e-05, 'b_lr': 1e-05, 'm_lr': 1e-05, 'b_lr_decay_rate': 0.6, 'word_drop_rate': 0.1, 'task_reward': 'logit', 'perfomance_reward_weight': 1, 'ctx_sim_reward_weight': 0.08}
 Test F1: 0.7567237163814182
Sub: True - Mul: True
 Best F1 MATRES: 0.783625730994152 
M 
F1: 0.783625730994152 
CM: 
 [[1072   60    0   10]
 [  89  653    0    9]
 [  46   29    0    6]
 [ 154   78    0   17]] 
Time: 2021-07-05 20:06:22.241425 

 -------------------------------------------- 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 512, 'p_mlp_dim': 768, 'n_head': 16, 'epoches': 9, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 3, 's_lr': 1e-05, 'b_lr': 1e-05, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.5, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 1, 'ctx_sim_reward_weight': 0.01}
 Test F1: 0.748166259168704
Sub: True - Mul: True
 Best F1 MATRES: 0.7666543982333456 
M 
F1: 0.7666543982333456 
CM: 
 [[1330   42    0    0]
 [ 197  753    0    0]
 [  67   22    0    0]
 [ 240   66    0    0]] 
Time: 2021-07-06 02:48:08.489012 

 -------------------------------------------- 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 768, 'p_mlp_dim': 768, 'n_head': 16, 'epoches': 7, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 3, 's_lr': 5e-05, 'b_lr': 5e-06, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.8, 'word_drop_rate': 0.1, 'task_reward': 'logit', 'perfomance_reward_weight': 0.1, 'ctx_sim_reward_weight': 0.01, 'knowledge_reward_weight': 0.1}
 Test F1: 0.7542787286063568
Sub: True - Mul: True
 Best F1 MATRES: 0.7818263607737292 
M 
F1: 0.7818263607737292 
CM: 
 [[1070   72    0    0]
 [  83  668    0    0]
 [  45   36    0    0]
 [ 157   92    0    0]] 
Time: 2021-07-07 01:28:20.362533 

 -------------------------------------------- 

 Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 768, 'p_mlp_dim': 512, 'epoches': 5, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 3, 's_lr': 5e-05, 'b_lr': 1e-06, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.8, 'word_drop_rate': 0.1, 'task_reward': 'logit', 'perfomance_reward_weight': 0.5, 'ctx_sim_reward_weight': 0.01, 'knowledge_reward_weight': 0.5}
 Test F1: 0.7518337408312957
Sub: True - Mul: True
 Best F1 MATRES: 0.7615870786516853 
M 
F1: 0.7615870786516853 
CM: 
 [[1362   99    0    0]
 [ 152  807    0    0]
 [  64   35    0    0]
 [ 213  116    0    0]] 
Time: 2021-07-07 14:01:22.808099 

 -------------------------------------------- 

 Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 512, 'p_mlp_dim': 512, 'epoches': 3, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 5, 's_lr': 1e-05, 'b_lr': 5e-06, 'm_lr': 0.0001, 'b_lr_decay_rate': 0.8, 'word_drop_rate': 0.01, 'task_reward': 'logit', 'perfomance_reward_weight': 1, 'ctx_sim_reward_weight': 0.01, 'knowledge_reward_weight': 0.5}
 Test F1: 0.7579462102689487
Sub: True - Mul: True
 Best F1 MATRES: 0.766502808988764 
M 
F1: 0.766502808988764 
CM: 
 [[1308  150    0    3]
 [  85  872    0    2]
 [  50   48    0    1]
 [ 166  160    0    3]] 
Time: 2021-07-07 15:38:31.455451 

 -------------------------------------------- 

 Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 512, 'p_mlp_dim': 512, 'epoches': 5, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 3, 's_lr': 0.0001, 'b_lr': 1e-05, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.6, 'word_drop_rate': 0.1, 'task_reward': 'logit', 'perfomance_reward_weight': 0.1, 'ctx_sim_reward_weight': 0.08, 'knowledge_reward_weight': 1}
 Test F1: 0.7616136919315404
Sub: True - Mul: True
 Best F1 MATRES: 0.7743040685224839 
M 
F1: 0.7743040685224839 
CM: 
 [[1114  104    0    5]
 [  75  694    0    0]
 [  38   44    0    2]
 [ 167   92    0    0]] 
Time: 2021-07-07 18:25:50.278211 

 -------------------------------------------- 

 Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 768, 'p_mlp_dim': 512, 'epoches': 5, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 1, 's_lr': 0.0001, 'b_lr': 1e-05, 'm_lr': 0.0001, 'b_lr_decay_rate': 0.5, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.1, 'ctx_sim_reward_weight': 0.05, 'knowledge_reward_weight': 1}
 Test F1: 0.7640586797066015
Sub: True - Mul: True
 Best F1 MATRES: 0.7559322033898305 
M 
F1: 0.7559322033898305 
CM: 
 [[1247   68    0   45]
 [  96  717    0   35]
 [  45   33    0    8]
 [ 212  106    0   43]] 
Time: 2021-07-07 21:04:31.071646 

 -------------------------------------------- 

 Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 512, 'p_mlp_dim': 1024, 'epoches': 7, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 5, 's_lr': 5e-05, 'b_lr': 5e-06, 'm_lr': 0.0001, 'b_lr_decay_rate': 0.7, 'word_drop_rate': 0.1, 'task_reward': 'logit', 'perfomance_reward_weight': 0.1, 'ctx_sim_reward_weight': 0.08, 'knowledge_reward_weight': 0.1}
 Test F1: 0.7506112469437652
Sub: True - Mul: True
 Best F1 MATRES: 0.7799783939503062 
M 
F1: 0.7799783939503062 
CM: 
 [[1318   79    0    0]
 [ 116  847    0    1]
 [  60   41    0    1]
 [ 210  103    0    1]] 
Time: 2021-07-08 00:34:24.321706 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 512, 'p_mlp_dim': 512, 'epoches': 7, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 3, 's_lr': 5e-05, 'b_lr': 1e-06, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.4, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.5, 'ctx_sim_reward_weight': 0.01, 'knowledge_reward_weight': 1}
 Test F1: 0.8567375886524823
Sub: True - Mul: True
 Best F1 MATRES: 0.8445552784704904 
M 
F1: 0.8445552784704904 
CM: 
 [[654  23   0]
 [118 362   0]
 [ 37   9   0]] 
Time: 2021-07-08 07:01:43.641130 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 512, 'p_mlp_dim': 768, 'epoches': 7, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 5, 's_lr': 5e-05, 'b_lr': 5e-06, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.4, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.5, 'ctx_sim_reward_weight': 0.08, 'knowledge_reward_weight': 0.1}
 Test F1: 0.8129117259552043
Sub: True - Mul: True
 Best F1 MATRES: 0.8188914910226386 
M 
F1: 0.8188914910226386 
CM: 
 [[631  44   0   2]
 [ 60 417   0   3]
 [ 31  14   1   0]
 [ 81  80   0   1]] 
Time: 2021-07-08 13:59:47.084461 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 768, 'p_mlp_dim': 768, 'epoches': 3, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 1, 's_lr': 1e-05, 'b_lr': 5e-06, 'm_lr': 0.0001, 'b_lr_decay_rate': 0.4, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.1, 'ctx_sim_reward_weight': 0.05, 'knowledge_reward_weight': 0.5}
 Test F1: 0.814765985497693
Sub: True - Mul: True
 Best F1 MATRES: 0.8334293948126802 
M 
F1: 0.8334293948126802 
CM: 
 [[460  24   0   4]
 [ 34 263   0   4]
 [ 18  15   0   2]
 [ 65  32   0  16]] 
Time: 2021-07-08 16:30:27.157898 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 768, 'p_mlp_dim': 512, 'epoches': 3, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 5, 's_lr': 0.0001, 'b_lr': 1e-06, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.6, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 1, 'ctx_sim_reward_weight': 0.08, 'knowledge_reward_weight': 1}
 Test F1: 0.7852921864740643
Sub: True - Mul: True
 Best F1 MATRES: 0.811023622047244 
M 
F1: 0.811023622047244 
CM: 
 [[713  52   0   0]
 [ 83 420   0   0]
 [ 27  22   0   0]
 [ 99  61   0   0]] 
Time: 2021-07-08 18:40:36.739805 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 768, 'p_mlp_dim': 1024, 'epoches': 3, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 1, 's_lr': 0.0001, 'b_lr': 1e-06, 'm_lr': 1e-05, 'b_lr_decay_rate': 0.6, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.1, 'ctx_sim_reward_weight': 0.01, 'knowledge_reward_weight': 0.5}
 Test F1: 0.7800393959290873
Sub: True - Mul: True
 Best F1 MATRES: 0.7879169288860919 
M 
F1: 0.7879169288860919 
CM: 
 [[747 104   0   0]
 [ 69 505   0   0]
 [ 31  24   0   0]
 [104 114   0   0]] 
Time: 2021-07-08 20:43:27.825055 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 512, 'p_mlp_dim': 512, 'epoches': 3, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 5, 's_lr': 0.0001, 'b_lr': 5e-06, 'm_lr': 1e-05, 'b_lr_decay_rate': 0.4, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.5, 'ctx_sim_reward_weight': 0.01, 'knowledge_reward_weight': 0.5}
 Test F1: 0.816137566137566
Sub: True - Mul: True
 Best F1 MATRES: 0.8066346573548667 
M 
F1: 0.8066346573548667 
CM: 
 [[583  38   0   4]
 [ 69 341   0   6]
 [ 19  13   0   3]
 [101  51   0   3]] 
Time: 2021-07-08 22:55:12.522913 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 512, 'p_mlp_dim': 512, 'epoches': 7, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 1, 's_lr': 0.0001, 'b_lr': 1e-06, 'm_lr': 1e-05, 'b_lr_decay_rate': 0.4, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 1, 'ctx_sim_reward_weight': 0.05, 'knowledge_reward_weight': 0.5}
 Test F1: 0.7918581746552857
Sub: True - Mul: True
 Best F1 MATRES: 0.7905723905723906 
M 
F1: 0.7905723905723906 
CM: 
 [[688 116   0   0]
 [ 50 486   0   0]
 [ 18  28   0   0]
 [ 89 109   0   0]] 
Time: 2021-07-09 02:48:45.140367 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 512, 'p_mlp_dim': 768, 'epoches': 7, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 5, 's_lr': 0.0001, 'b_lr': 1e-05, 'm_lr': 0.0001, 'b_lr_decay_rate': 0.8, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.1, 'ctx_sim_reward_weight': 0.03, 'knowledge_reward_weight': 0.7}
 Test F1: 0.8097165991902834
Sub: True - Mul: True
 Best F1 MATRES: 0.8044638524987872 
M 
F1: 0.8044638524987872 
CM: 
 [[516  34   0  14]
 [ 42 313   1   4]
 [ 21  25   0   5]
 [ 82  50   2  14]] 
Time: 2021-07-09 06:57:10.938337 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 512, 'p_mlp_dim': 512, 'epoches': 5, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 1, 's_lr': 0.0001, 'b_lr': 1e-06, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.6, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.1, 'ctx_sim_reward_weight': 0.05, 'knowledge_reward_weight': 0.5}
 Test F1: 0.8063033486539725
Sub: True - Mul: True
 Best F1 MATRES: 0.8465373961218837 
M 
F1: 0.8465373961218837 
CM: 
 [[447  21   0   0]
 [ 29 317   0   0]
 [ 20  13   0   0]
 [ 69  42   0   0]] 
Time: 2021-07-09 09:51:44.847980 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 768, 'p_mlp_dim': 1024, 'epoches': 5, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 3, 's_lr': 1e-05, 'b_lr': 5e-06, 'm_lr': 0.0001, 'b_lr_decay_rate': 0.6, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 1, 'ctx_sim_reward_weight': 0.08, 'knowledge_reward_weight': 0.1}
 Test F1: 0.8134034165571616
Sub: True - Mul: True
 Best F1 MATRES: 0.847227191413238 
M 
F1: 0.847227191413238 
CM: 
 [[688  35   0   0]
 [ 52 496   0   0]
 [ 28  20   0   0]
 [ 97  60   0   0]] 
Time: 2021-07-09 12:45:50.333168 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 512, 'p_mlp_dim': 512, 'epoches': 7, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 5, 's_lr': 1e-05, 'b_lr': 1e-05, 'm_lr': 0.0001, 'b_lr_decay_rate': 0.6, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.1, 'ctx_sim_reward_weight': 0.01, 'knowledge_reward_weight': 0.1}
 Test F1: 0.8107023411371237
Sub: True - Mul: True
 Best F1 MATRES: 0.8407871198568874 
M 
F1: 0.8407871198568874 
CM: 
 [[436  23   0  12]
 [ 37 268   0  11]
 [ 13  14   1   2]
 [ 47  21   0   6]] 
Time: 2021-07-09 16:38:51.646337 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 768, 'p_mlp_dim': 1024, 'epoches': 5, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 3, 's_lr': 1e-05, 'b_lr': 5e-06, 'm_lr': 0.0001, 'b_lr_decay_rate': 0.8, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.7, 'ctx_sim_reward_weight': 0.08, 'knowledge_reward_weight': 0.1}
 Test F1: 0.8010505581089954
Sub: True - Mul: True
 Best F1 MATRES: 0.7993366500829188 
M 
F1: 0.7993366500829188 
CM: 
 [[412  60   0   0]
 [ 22 311   0   0]
 [ 14  21   0   0]
 [ 51  78   0   0]] 
Time: 2021-07-09 19:36:16.929647 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 768, 'p_mlp_dim': 1024, 'epoches': 5, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 3, 's_lr': 1e-05, 'b_lr': 1e-06, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.6, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 1, 'ctx_sim_reward_weight': 0.05, 'knowledge_reward_weight': 1}
 Test F1: 0.803676953381484
Sub: True - Mul: True
 Best F1 MATRES: 0.8113289760348583 
M 
F1: 0.8113289760348583 
CM: 
 [[651  38   0   0]
 [ 65 280   0   0]
 [ 30  14   0   0]
 [ 85  54   0   0]] 
Time: 2021-07-09 22:31:59.284564 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 512, 'p_mlp_dim': 1024, 'epoches': 5, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1}, 'num_ctx_select': 3, 's_lr': 5e-05, 'b_lr': 1e-06, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.6, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.7, 'ctx_sim_reward_weight': 0.03, 'knowledge_reward_weight': 0.7}
 Test F1: 0.7971109652002626
Sub: True - Mul: True
 Best F1 MATRES: 0.7961399276236429 
M 
F1: 0.7961399276236429 
CM: 
 [[625  61   0   0]
 [ 73 365   0   0]
 [ 29  15   0   0]
 [ 89  62   0   0]] 
Time: 2021-07-10 01:27:16.971592 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 768, 'p_mlp_dim': 1024, 'epoches': 3, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 5, 's_lr': 0.0001, 'b_lr': 3e-06, 'm_lr': 3e-05, 'b_lr_decay_rate': 0.3, 'word_drop_rate': 0.1, 'task_reward': 'logit', 'perfomance_reward_weight': 0.1, 'ctx_sim_reward_weight': 0.01, 'knowledge_reward_weight': 0.5}
 Test F1: 0.803676953381484
Sub: True - Mul: True
 Best F1 MATRES: 0.7990654205607477 
M 
F1: 0.7990654205607477 
CM: 
 [[622  55   0   0]
 [ 76 404   0   0]
 [ 30  16   0   0]
 [ 98  64   0   0]] 
Time: 2021-07-10 05:01:50.220768 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 512, 'p_mlp_dim': 512, 'epoches': 5, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 3, 's_lr': 5e-05, 'b_lr': 7e-06, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.3, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 1, 'ctx_sim_reward_weight': 0.03, 'knowledge_reward_weight': 1}
 Test F1: 0.8182446440912233
Sub: True - Mul: True
 Best F1 MATRES: 0.8223760092272202 
M 
F1: 0.8223760092272202 
CM: 
 [[444  31   0  36]
 [ 24 269   0  23]
 [ 11  11   0   6]
 [ 53  36   0  28]] 
Time: 2021-07-10 07:51:41.673885 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 768, 'p_mlp_dim': 512, 'epoches': 3, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 5, 's_lr': 0.0001, 'b_lr': 7e-06, 'm_lr': 7e-05, 'b_lr_decay_rate': 0.3, 'word_drop_rate': 0.1, 'task_reward': 'logit', 'perfomance_reward_weight': 0.7, 'ctx_sim_reward_weight': 0.03, 'knowledge_reward_weight': 0.7}
 Test F1: 0.8115561391989494
Sub: True - Mul: True
 Best F1 MATRES: 0.817762399077278 
M 
F1: 0.817762399077278 
CM: 
 [[405  28   0   0]
 [ 43 304   0   0]
 [ 18   8   0   0]
 [ 80  42   0   0]] 
Time: 2021-07-10 09:58:35.055702 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 768, 'p_mlp_dim': 1024, 'epoches': 5, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 5, 's_lr': 5e-05, 'b_lr': 5e-06, 'm_lr': 3e-05, 'b_lr_decay_rate': 0.4, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.5, 'ctx_sim_reward_weight': 0.03, 'knowledge_reward_weight': 0.1}
 Test F1: 0.8187211601845747
Sub: True - Mul: True
 Best F1 MATRES: 0.8194085558426644 
M 
F1: 0.8194085558426644 
CM: 
 [[893  68   0   7]
 [ 74 534   0   2]
 [ 34  24   0   1]
 [122  97   0   3]] 
Time: 2021-07-10 12:46:00.846135 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 512, 'p_mlp_dim': 768, 'epoches': 7, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 3, 's_lr': 5e-05, 'b_lr': 3e-06, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.5, 'word_drop_rate': 0.1, 'task_reward': 'logit', 'perfomance_reward_weight': 0.7, 'ctx_sim_reward_weight': 0.03, 'knowledge_reward_weight': 0.7}
 Test F1: 0.8165680473372782
Sub: True - Mul: True
 Best F1 MATRES: 0.8163123039386546 
M 
F1: 0.8163123039386546 
CM: 
 [[683  80   0   1]
 [ 57 488   0   0]
 [ 29  22   0   0]
 [ 83  67   0   0]] 
Time: 2021-07-10 16:37:20.344077 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 512, 'p_mlp_dim': 768, 'epoches': 3, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 5, 's_lr': 5e-05, 'b_lr': 5e-06, 'm_lr': 3e-05, 'b_lr_decay_rate': 0.5, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.7, 'ctx_sim_reward_weight': 0.05, 'knowledge_reward_weight': 0.1}
 Test F1: 0.814182534471438
Sub: True - Mul: True
 Best F1 MATRES: 0.7857517024620221 
M 
F1: 0.7857517024620221 
CM: 
 [[462  44   0   1]
 [ 57 288   0   0]
 [ 21  18   0   0]
 [ 85  43   0   1]] 
Time: 2021-07-10 19:06:16.745862 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 512, 'p_mlp_dim': 768, 'epoches': 3, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 1, 's_lr': 5e-05, 'b_lr': 7e-06, 'm_lr': 3e-05, 'b_lr_decay_rate': 0.6, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.5, 'ctx_sim_reward_weight': 0.03, 'knowledge_reward_weight': 0.7}
 Test F1: 0.8148639681486397
Sub: True - Mul: True
 Best F1 MATRES: 0.8101367658889782 
M 
F1: 0.8101367658889782 
CM: 
 [[661  40   0   0]
 [ 62 346   0   0]
 [ 36  17   0   0]
 [114  48   0   0]] 
Time: 2021-07-10 21:27:44.483099 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 768, 'p_mlp_dim': 512, 'epoches': 5, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 1, 's_lr': 0.0001, 'b_lr': 7e-06, 'm_lr': 5e-05, 'b_lr_decay_rate': 0.3, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 1, 'ctx_sim_reward_weight': 0.01, 'knowledge_reward_weight': 1}
 Test F1: 0.8137651821862348
Sub: True - Mul: True
 Best F1 MATRES: 0.8161389172625128 
M 
F1: 0.8161389172625128 
CM: 
 [[506  32   0  13]
 [ 32 293   0  15]
 [ 27  18   0   4]
 [ 66  44   0  12]] 
Time: 2021-07-11 00:14:50.789783 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 512, 'p_mlp_dim': 1024, 'epoches': 3, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 5, 's_lr': 5e-05, 'b_lr': 7e-06, 'm_lr': 3e-05, 'b_lr_decay_rate': 0.3, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.5, 'ctx_sim_reward_weight': 0.03, 'knowledge_reward_weight': 0.7}
 Test F1: 0.8222811671087534
Sub: True - Mul: True
 Best F1 MATRES: 0.8057798555036123 
M 
F1: 0.8057798555036123 
CM: 
 [[611  42   0  13]
 [ 53 337   0   9]
 [ 31  20   0   1]
 [ 79  63   0  20]] 
Time: 2021-07-11 02:42:20.224730 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 768, 'p_mlp_dim': 512, 'epoches': 3, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 5, 's_lr': 5e-05, 'b_lr': 5e-06, 'm_lr': 3e-05, 'b_lr_decay_rate': 0.6, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.5, 'ctx_sim_reward_weight': 0.01, 'knowledge_reward_weight': 0.1}
 Test F1: 0.8156650911546253
Sub: True - Mul: True
 Best F1 MATRES: 0.8280329799764429 
M 
F1: 0.8280329799764429 
CM: 
 [[861  62   0  23]
 [ 53 545   0  25]
 [ 36  23   0   7]
 [107  74   0  33]] 
Time: 2021-07-11 05:05:59.115976 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 512, 'p_mlp_dim': 1024, 'epoches': 7, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 5, 's_lr': 5e-05, 'b_lr': 7e-06, 'm_lr': 7e-05, 'b_lr_decay_rate': 0.4, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.5, 'ctx_sim_reward_weight': 0.08, 'knowledge_reward_weight': 0.7}
 Test F1: 0.8071099407504938
Sub: True - Mul: True
 Best F1 MATRES: 0.8198449612403101 
M 
F1: 0.8198449612403101 
CM: 
 [[765  85   0   0]
 [ 41 557   0   0]
 [ 28  31   0   0]
 [101 110   0   0]] 
Time: 2021-07-11 08:58:52.721083 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 768, 'p_mlp_dim': 1024, 'epoches': 5, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 5, 's_lr': 5e-05, 'b_lr': 5e-06, 'm_lr': 3e-05, 'b_lr_decay_rate': 0.4, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.5, 'ctx_sim_reward_weight': 0.03, 'knowledge_reward_weight': 0.1}
 Test F1: 0.8181221273801708
Sub: True - Mul: True
 Best F1 MATRES: 0.8386167146974063 
M 
F1: 0.8386167146974063 
CM: 
 [[505  43   0   0]
 [ 36 368   0   0]
 [ 18  15   0   0]
 [ 62  50   0   0]] 
Time: 2021-07-11 11:52:49.944671 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 768, 'p_mlp_dim': 1024, 'epoches': 5, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 5, 's_lr': 5e-05, 'b_lr': 5e-06, 'm_lr': 3e-05, 'b_lr_decay_rate': 0.4, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.5, 'ctx_sim_reward_weight': 0.08, 'knowledge_reward_weight': 0.5}
 Test F1: 0.8102429415627052
Sub: True - Mul: True
 Best F1 MATRES: 0.8171861836562763 
M 
F1: 0.8171861836562763 
CM: 
 [[592  36   0   1]
 [ 67 378   0   2]
 [ 32  13   0   1]
 [ 82  52   0   1]] 
Time: 2021-07-11 14:45:50.985555 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 512, 'p_mlp_dim': 1024, 'epoches': 5, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 5, 's_lr': 5e-05, 'b_lr': 5e-06, 'm_lr': 3e-05, 'b_lr_decay_rate': 0.4, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.1, 'ctx_sim_reward_weight': 0.05, 'knowledge_reward_weight': 0.1}
 Test F1: 0.8160535117056856
Sub: True - Mul: True
 Best F1 MATRES: 0.8359114346375492 
M 
F1: 0.8359114346375492 
CM: 
 [[824  39   0  37]
 [ 60 554   0  48]
 [ 39  15   0   9]
 [ 85  56   0  41]] 
Time: 2021-07-11 17:53:18.756581 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 512, 'p_mlp_dim': 1024, 'epoches': 5, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 5, 's_lr': 5e-05, 'b_lr': 5e-06, 'm_lr': 3e-05, 'b_lr_decay_rate': 0.4, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.5, 'ctx_sim_reward_weight': 0.03, 'knowledge_reward_weight': 0.7}
 Test F1: 0.8152531229454306
Sub: True - Mul: True
 Best F1 MATRES: 0.8121567191504943 
M 
F1: 0.8121567191504943 
CM: 
 [[670  62   0   0]
 [ 53 439   0   1]
 [ 30  24   0   0]
 [105  69   0   2]] 
Time: 2021-07-11 21:02:54.641223 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 768, 'p_mlp_dim': 1024, 'epoches': 7, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 3, 's_lr': 5e-05, 'b_lr': 3e-06, 'm_lr': 7e-05, 'b_lr_decay_rate': 0.3, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.5, 'ctx_sim_reward_weight': 0.03, 'knowledge_reward_weight': 0.1}
 Test F1: 0.8147174770039421
Sub: True - Mul: True
 Best F1 MATRES: 0.8150208623087621 
M 
F1: 0.8150208623087621 
CM: 
 [[470  58   0   1]
 [ 34 409   0   0]
 [ 18  22   0   0]
 [ 64  70   0   0]] 
Time: 2021-07-12 00:35:35.993524 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 512, 'p_mlp_dim': 1024, 'epoches': 3, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 1, 's_lr': 0.0001, 'b_lr': 7e-06, 'm_lr': 3e-05, 'b_lr_decay_rate': 0.3, 'word_drop_rate': 0.1, 'task_reward': 'logit', 'perfomance_reward_weight': 0.5, 'ctx_sim_reward_weight': 0.03, 'knowledge_reward_weight': 0.7}
 Test F1: 0.7971109652002626
Sub: True - Mul: True
 Best F1 MATRES: 0.7946188340807175 
M 
F1: 0.7946188340807175 
CM: 
 [[808  45   0   0]
 [120 521   0   0]
 [ 37  22   0   0]
 [149  90   0   0]] 
Time: 2021-07-12 02:32:41.994718 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-base
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 768, 'p_mlp_dim': 512, 'epoches': 7, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 3, 's_lr': 5e-05, 'b_lr': 0.0001, 'm_lr': 7e-05, 'b_lr_decay_rate': 0.5, 'word_drop_rate': 0.1, 'task_reward': 'logit', 'perfomance_reward_weight': 0.1, 'ctx_sim_reward_weight': 0.01, 'knowledge_reward_weight': 0.5}
 Test F1: 0.6514423076923076
Sub: True - Mul: TrueT 
F1: 0.6004842615012107 
CM: 
 [[114  12   3   1   1  28]
 [  9 118   5   7   0  42]
 [  2   1   6   0   1   6]
 [  1   0   0   8   1  15]
 [  5   0   2   1   2  18]
 [ 42  37  13  24   1 137]] 
Time: 2021-07-12 03:34:05.112714 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-base
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 768, 'p_mlp_dim': 1024, 'epoches': 3, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 3, 's_lr': 5e-05, 'b_lr': 0.0001, 'm_lr': 3e-05, 'b_lr_decay_rate': 0.5, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.1, 'ctx_sim_reward_weight': 0.03, 'knowledge_reward_weight': 0.1}
 Test F1: 0.6342364532019703
Sub: True - Mul: TrueT 
F1: 0.5871313672922253 
CM: 
 [[102   5   0   0   0  52]
 [  7 108   2   5   0  59]
 [  1   0   4   0   1  10]
 [  1   0   0   4   1  19]
 [  4   1   1   0   1  21]
 [ 41  28   8  12   0 165]] 
Time: 2021-07-12 04:07:49.760647 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-base
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 768, 'p_mlp_dim': 768, 'epoches': 7, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 1, 's_lr': 5e-05, 'b_lr': 0.0001, 'm_lr': 3e-05, 'b_lr_decay_rate': 0.3, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.7, 'ctx_sim_reward_weight': 0.01, 'knowledge_reward_weight': 0.5}
 Test F1: 0.6275773195876289
Sub: True - Mul: TrueT 
F1: 0.6191117092866756 
CM: 
 [[108   5   0   0   0  46]
 [  6 120   0   0   0  55]
 [  1   1   0   0   1  13]
 [  2   0   0   1   0  22]
 [  3   1   0   0   1  23]
 [ 43  31   2   8   0 170]] 
Time: 2021-07-12 04:38:22.620390 

 -------------------------------------------- 

Note: no use lstm in predictor 
roberta-base
Hypeparameter: 
{'s_hidden_dim': 256, 's_mlp_dim': 768, 'p_mlp_dim': 768, 'epoches': 3, 'warming_epoch': 0, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 3, 's_lr': 5e-05, 'b_lr': 0.0001, 'm_lr': 3e-05, 'b_lr_decay_rate': 0.3, 'word_drop_rate': 0.1, 'task_reward': 'logit', 'perfomance_reward_weight': 0.5, 'ctx_sim_reward_weight': 0.03, 'knowledge_reward_weight': 1}
 Test F1: 0.4446215139442231
Sub: True - Mul: TrueT 
F1: 0.3623931623931624 
CM: 
 [[ 93   1   0   0   0  65]
 [  9  13   0   0   0 159]
 [  1   0   0   0   0  15]
 [  3   0   0   0   0  22]
 [  5   0   0   0   0  23]
 [ 47   4   0   0   0 203]] 
Time: 2021-07-12 04:57:31.390587 

 -------------------------------------------- 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 512, 'p_mlp_dim': 1024, 'epoches': 3, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 5, 's_lr': 5e-05, 'b_lr': 7e-06, 'm_lr': 3e-05, 'b_lr_decay_rate': 0.3, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.0, 'ctx_sim_reward_weight': 0.05, 'knowledge_reward_weight': 0.7, 'is_lstm': False, 'threshold': 1, 'seed': 1741}
 Test F1: 0.8134680134680136
Seed: 1741
Activate function: relu6
Sub: True - Mul: TrueM 
F1: 0.8150256118383609 
CM: 
 [[490  28   0  18]
 [ 20 226   0  15]
 [ 17  17   0   3]
 [ 70  55   0  15]] 
Time: 2021-07-20 10:32:27.027983 

 -------------------------------------------- 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 512, 'p_mlp_dim': 1024, 'epoches': 3, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 5, 's_lr': 5e-05, 'b_lr': 7e-06, 'm_lr': 3e-05, 'b_lr_decay_rate': 0.3, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.0, 'ctx_sim_reward_weight': 0.03, 'knowledge_reward_weight': 0.7, 'is_lstm': False, 'threshold': 1, 'seed': 1741}
 Test F1: 0.8180610889774237
Seed: 1741
Activate function: relu
Sub: True - Mul: TrueM 
F1: 0.8200789622109419 
CM: 
 [[497  29   0  10]
 [ 24 230   0   7]
 [ 17  16   0   4]
 [ 70  56   0  14]] 
Time: 2021-07-20 13:21:36.077613 

 -------------------------------------------- 
roberta-large
Hypeparameter: 
{'s_hidden_dim': 512, 's_mlp_dim': 512, 'p_mlp_dim': 1024, 'epoches': 3, 'warming_epoch': 1, 'task_weights': {'1': 1, '2': 1, '3': 1, '4': 1}, 'num_ctx_select': 5, 's_lr': 5e-05, 'b_lr': 7e-06, 'm_lr': 3e-05, 'b_lr_decay_rate': 0.3, 'word_drop_rate': 0.05, 'task_reward': 'logit', 'perfomance_reward_weight': 0.0, 'ctx_sim_reward_weight': 0.03, 'knowledge_reward_weight': 0.7, 'is_lstm': False, 'threshold': 1, 'seed': 1741}
 Test F1: 0.7989487516425756
Seed: 1741
Activate function: silu
Sub: True - Mul: FalseM 
F1: 0.7957941339236303 
CM: 
 [[515  20   0   1]
 [ 57 204   0   0]
 [ 21  16   0   0]
 [ 96  44   0   0]] 
Time: 2021-07-22 03:07:41.987798 
